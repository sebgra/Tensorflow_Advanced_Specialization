# TensorFlow Advanced Technic Specialization - Coursera

This repo contain all the materials from [Coursera](https://www.coursera.org/specializations/tensorflow-advanced-techniques).

## C1 :  Custom Models, Layers, and Loss function with TensorFlow

### Week 1 : Functional APIs

- [Functional API](./C1/week_1/C1_W1_Lab_1_functional-practice.ipynb)
- [Functional API usage](./C1/week_1/C1_W1_Lab_2_multi-output.ipynb)
- [Siamese Network](./C1/week_1/C1_W1_Lab_3_siamese-network.ipynb)
- [Assignment](./C1/week_1/C1W1_Assignment.ipynb)


### Week 2 : Custom Loss functions

- [Custom Loss functions](./C1/week_2/C1_W2_Lab_1_huber-loss.ipynb)
- [Custom Loss Class and hyperparameters](./C1/week_2/C1_W2_Lab_2_huber-object-loss.ipynb)
- [Contrastive Loss](./C1/week_2/C1_W1_Lab_3_siamese-network.ipynb)
- [Assignment](./C1/week_2/C1W2_Assignment.ipynb)



### Week 3 : Custom Layers

- [Custom Lambda Layers](./C1/week_3/C1_W3_Lab_1_lambda-layer.ipynb)
- [Custom Lambda Layers : Usage](./C1/week_3/C1_W3_Lab_2_custom-dense-layer.ipynb)
- [Activation of custom layers](./C1/week_3/C1_W3_Lab_3_custom-layer-activation.ipynb)
- [Assignment](./C1/week_3/C1W3_Assignment.ipynb)


### Week 4 : Custom Models

- [Complex architectures with the functional API](./C1/week_4/C1_W4_Lab_1_basic-model.ipynb )
- Simplify complex architectures with class Model
- [RestNet implementation](./C1/week_4/C1_W4_Lab_2_resnet-example.ipynb)
- [Assignment](./C1/week_4/C1W4_Assignment.ipynb)


### Week 5 : Callbacks

- [Integrated reminder](./C1/week_5/C1_W5_Lab_1_exploring-callbacks.ipynb)
- [Personnalized reminder](./C1/week_5/C1_W5_Lab_2_custom-callbacks.ipynb)

## C2 : Custom and distributed  Training with TensorFlow

### Week 1 : Gradients and differenciation 

- [Tensor Notions](./C2/week_1/C2_W1_Lab_1_basic-tensors.ipynb)
- [Tensor in eager mode](./C2/week_1/C2_W1_Lab_2_gradient-tape-basics.ipynb)
- [Assingment](./C2/week_1/C2W1_Assignment.ipynb)

### Week 2 : Custom functions

- [Custom training loops](./C2/week_2/C2_W2_Lab_1_training-basics.ipynb)
- [Custom pipeline with TensorFlow](./C2/week_2/C2_W2_Lab_2_training-categorical.ipynb)
- [Assignment](./C2/week_2/C2W2_Assignment.ipynb)

### Week 3 : Graphic mode

- [Autograph](./C2/week_3/C2_W3_Lab_1_autograph-basics.ipynb)
- [Complex code graph building](./C2/week_3/C2_W3_Lab_2-graphs-for-complex-code.ipynb)
- [Assignment](./C2/week_3/C2W3_Assignment.ipynb)

### Week 4 : Distributed Strategy

- [Mirror strategy](./C2/week_4/C2_W4_Lab_1_basic-mirrored-strategy.ipynb)
- [Mirror multi-GPU Strategy](./C2/week_4/C2_W4_Lab_2_multi-GPU-mirrored-strategy.ipynb)
- [TPU strategy](./C2/week_4/C2_W4_Lab_3_using-TPU-strategy.ipynb)()
- [TPU strategy 2](./C2/week_4/C2_W4_Lab_4_one-device-strategy.ipynb)()
- [Assignment](./C2/week_4/C2W4_Assignment.ipynb)

## C3 : Advanced computer vision ith TensorFlow

### Week 1 : Introduction to computer Vision

- [Transfert learning](./C3/week_1/C3_W1_Lab_1_transfer_learning_cats_dogs.ipynb)
- [Advanced transfert learning](./C3/week_1/C3_W1_Lab_2_Transfer_Learning_CIFAR_10.ipynb)
- [Detection and object localization](./C3/week_1/C3_W1_Lab_3_Object_Localization.ipynb)
- [Assignment](./C3/week_1/C3W1_Assignment.ipynb)

### Week 2 : Object detection

- [Object detection with TensorFlow](./C3/week_2/C3_W2_Lab_1_Simple_Object_Detection.ipynb)
- [Object detection API](./C3/week_2/C3_W2_Lab_2_Object_Detection.ipynb)
- [Assignment](./C3/week_2/C3W2_Assignment.ipynb)

### Week 3 : Image segentation

- [Image segmentation overview : FCNN](./C3/week_3/C3_W3_Lab_1_VGG16-FCN8-CamVid.ipynb)
- [U-Net](./C3/week_3/C3_W3_Lab_2_OxfordPets_UNet.ipynb)
- [Instance segmentation](./C3/week_3/C3_W3_Lab_3_Mask_RCNN_ImageSegmentation.ipynb)
- [Assignment](./C3/week_3/C3W3_Assignment.ipynb)

### Week 4 : Models visualization and interpretability

- [Introduction to visualization and interpretability](./C3/week_4/C3_W4_Lab_1_FashionMNIST_CAM.ipynb)
- [Introduction to visualization and interpretability 2](./C3/week_4/C3_W4_Lab_2_CatsDogs_CAM.ipynb)
- [Saliency maps](./C3/week_4/C3_W4_Lab_3_Saliency.ipynb)
- [Gradients and Class Activation Maps](./C3/week_4/C3_W4_Lab_4_GradCam.ipynb)
- [Assignment](./C3/week_4/C3W4_Assignment.ipynb)

## C4 : Generative deep learning with TensorFlow

### Week 1 : Style transfert

- [Neural Style Tranfert](./C4/week_1/C4_W2_Lab_1_FirstAutoEncoder.ipynb)
- [Neural Style Tranfert Pt2](./C4/week_1/C4_W1_Lab_2_Fast_NST.ipynb)
- [Fast Neural Style Tranfert](./C4/week_1/C4_W1_Lab_2_Fast_NST.ipynb)
- [Assignment](./C4/week_1/C4W1_Assignment.ipynb)

### Week 2 : Auto-Encoders

- [First Auto-Encoder](./C4/week_2/C4_W2_Lab_1_FirstAutoEncoder.ipynb)
- [First Auto-Encoder MNIST](./C4/week_2/C4_W2_Lab_2_MNIST_Autoencoder.ipynb)
- [Deep Auto-Encoder](./C4/week_2/C4_W2_Lab_3_MNIST_DeepAutoencoder.ipynb)
- [CNN Auto-Encoder](./C4/week_2/C4_W2_Lab_4_FashionMNIST_CNNAutoEncoder.ipynb)
- [Noisy CNN Auto-Encoder](./C4/week_2/C4_W2_Lab_5_FashionMNIST_NoisyCNNAutoEncoder.ipynb)
- [Assignment](./C4/week_2/C4W2_Assignment.ipynb)

### Week 3 : Variational Auto-Encoders

- [Variational Auto-Encoder MNIST](./C4/week_3/C4_W3_Lab_1_VAE_MNIST.ipynb)
- [Assignment](./C4/week_3/C4W3_Assignment.ipynb)

### Week 4 : GANs

- [First GAN](./C4/week_4/C4_W4_Lab_1_First_GAN.ipynb)
- [DCGAN](./C4/week_4/C4_W4_Lab_2_First_DCGAN.ipynb)
- [CelabA GAN](./C4/week_4/C4_W4_Lab_3_CelebA_GAN_Experiments.ipynb)
- [Assignment](./C4/week_4/C4W4_Assignment.ipynb)

# Snipets

## Creating models with Sequential

```python
tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28, 28)),
                                            tf.keras.layers.Dense(128, activation=tf.nn.relu),
                                            tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
```

## Creting models with API

```python

# instantiate the input Tensor
from tensorflow.keras.models import Model

input_layer = tf.keras.Input(shape=(28, 28))

# stack the layers using the syntax: new_layer()(previous_layer)
flatten_layer = tf.keras.layers.Flatten()(input_layer)
first_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)(flatten_layer)
output_layer = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(first_dense)

# declare inputs and outputs
func_model = Model(inputs=input_layer, outputs=output_layer)
```


## Plot Model

```python
from tensorflow.python.keras.utils.vis_utils import plot_model

plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')
```

## Build model and train

```python
# configure, train, and evaluate the model
model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(training_images, training_labels, epochs=5)
model.evaluate(test_images, test_labels)
```
## Multiple Ouput Models

```python
#Define model layers.
input_layer = Input(shape=(len(train .columns),))
first_dense = Dense(units='128', activation='relu')(input_layer)
second_dense = Dense(units='128', activation='relu')(first_dense)

# Y1 output will be fed directly from the second dense
y1_output = Dense(units='1', name='y1_output')(second_dense)
third_dense = Dense(units='64', activation='relu')(second_dense)

# Y2 output will come via the third dense
y2_output = Dense(units='1', name='y2_output')(third_dense)

# Define the model with the input layer and a list of output layers
model = Model(inputs=input_layer, outputs=[y1_output, y2_output])

# Specify the optimizer, and compile the model with loss functions for both outputs
optimizer = tf.keras.optimizers.SGD(lr=0.001)
model.compile(optimizer=optimizer,
              loss={'y1_output': 'mse', 'y2_output': 'mse'},
              metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),
                       'y2_output': tf.keras.metrics.RootMeanSquaredError()})
# Train the model for 500 epochs
history = model.fit(norm_train_X, train_Y,
                    epochs=500, batch_size=10, validation_data=(norm_test_X, test_Y))

# Test the model and print loss and mse for both outputs
loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)
print("Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))
```

## Multiple Input Model - Siamese NN

```python
# create the left input and point to the base network
input_a = Input(shape=(28,28,), name="left_input")
vect_output_a = base_network(input_a)

# create the right input and point to the base network
input_b = Input(shape=(28,28,), name="right_input")
vect_output_b = base_network(input_b)

# measure the similarity of the two vector outputs
output = Lambda(euclidean_distance, name="output_layer", output_shape=eucl_dist_output_shape)([vect_output_a, vect_output_b])

# specify the inputs and output of the model
model = Model([input_a, input_b], output)

# plot model graph
plot_model(model, show_shapes=True, show_layer_names=True, to_file='outer-model.png')
```

## Custom Loss function

```python
def my_huber_loss(y_true, y_pred):
    threshold = 1
    error = y_true - y_pred
    is_small_error = tf.abs(error) <= threshold
    small_error_loss = tf.square(error) / 2
    big_error_loss = threshold * (tf.abs(error) - (0.5 * threshold))
    return tf.where(is_small_error, small_error_loss, big_error_loss)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss=my_huber_loss)
model.fit(xs, ys, epochs=500,verbose=0)
```

## Custom loss function with hyperparameter

```python
from tensorflow.keras.losses import Loss

class MyHuberLoss(Loss):
  
    # initialize instance attributes
    def __init__(self, threshold=1):
        super().__init__()
        self.threshold = threshold

    # compute loss
    def call(self, y_true, y_pred):
        error = y_true - y_pred
        is_small_error = tf.abs(error) <= self.threshold
        small_error_loss = tf.square(error) / 2
        big_error_loss = self.threshold * (tf.abs(error) - (0.5 * self.threshold))
        return tf.where(is_small_error, small_error_loss, big_error_loss)

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer='sgd', loss=MyHuberLoss(threshold=1.02))
model.fit(xs, ys, epochs=500,verbose=0)
```


## Custom Lambda Layers

```python
def my_relu(x):
    return K.maximum(-0.1, x)

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128),
    tf.keras.layers.Lambda(my_relu), 
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
```

## Custom Layer

```python
from tensorflow.keras.layers import Layer

class SimpleDense(Layer):

    def __init__(self, units=32):
        '''Initializes the instance attributes'''
        super(SimpleDense, self).__init__()
        self.units = units

    def build(self, input_shape):
        '''Create the state of the layer (weights)'''
        # initialize the weights
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(name="kernel",
            initial_value=w_init(shape=(input_shape[-1], self.units),
                                 dtype='float32'),
            trainable=True)

        # initialize the biases
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(name="bias",
            initial_value=b_init(shape=(self.units,), dtype='float32'),
            trainable=True)

    def call(self, inputs):
        '''Defines the computation from inputs to outputs'''
        return tf.matmul(inputs, self.w) + self.b

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    SimpleDense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

## Complex Architecture

```python
# inherit from the Model base class
class WideAndDeepModel(Model):
    def __init__(self, units=30, activation='relu', **kwargs):
        '''initializes the instance attributes'''
        super().__init__(**kwargs)
        self.hidden1 = Dense(units, activation=activation)
        self.hidden2 = Dense(units, activation=activation)
        self.main_output = Dense(1)
        self.aux_output = Dense(1)

    def call(self, inputs):
        '''defines the network architecture'''
        input_A, input_B = inputs
        hidden1 = self.hidden1(input_B)
        hidden2 = self.hidden2(hidden1)
        concat = concatenate([input_A, hidden2])
        main_output = self.main_output(concat)
        aux_output = self.aux_output(hidden2)
        
        return main_output, aux_output

# create an instance of the model
model = WideAndDeepModel()
```

## Custom Callbacks

```python
callback = tf.keras.callbacks.LambdaCallback(
    on_epoch_end=lambda epoch,logs: 
    print("Epoch: {}, Val/Train loss ratio: {:.2f}".format(epoch, logs["val_loss"] / logs["loss"]))
)

model = get_model()
_ = model.fit(x_train, y_train,
          validation_data=(x_test, y_test),
          batch_size=64,
          epochs=3,
          verbose=0,
          callbacks=[callback])
```

## Gradient and differencition

```python
x = tf.Variable(1.0)

with tf.GradientTape() as tape_2:
    with tf.GradientTape() as tape_1:
        y = x * x * x

        dy_dx = tape_1.gradient(y, x)
        
        # this is acceptable
        d2y_dx2 = tape_2.gradient(dy_dx, x)

print(dy_dx)
print(d2y_dx2)
```

## Custom training loops

```python
def base_model():
  inputs = tf.keras.Input(shape=(784,), name='digits')
  x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)
  x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)
  outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)
  model = tf.keras.Model(inputs=inputs, outputs=outputs)
  return model

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()
val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()

def apply_gradient(optimizer, model, x, y):
  with tf.GradientTape() as tape:
    logits = model(x)
    loss_value = loss_object(y_true=y, y_pred=logits)
  
  gradients = tape.gradient(loss_value, model.trainable_weights)
  optimizer.apply_gradients(zip(gradients, model.trainable_weights))
  
  return logits, loss_value

def train_data_for_one_epoch():
  losses = []
  pbar = tqdm(total=len(list(enumerate(train))), position=0, leave=True, bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} ')
  for step, (x_batch_train, y_batch_train) in enumerate(train):
      logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)
      
      losses.append(loss_value)
      
      train_acc_metric(y_batch_train, logits)
      pbar.set_description("Training loss for step %s: %.4f" % (int(step), float(loss_value)))
      pbar.update()
  return losses

def perform_validation():
  losses = []
  for x_val, y_val in test:
      val_logits = model(x_val)
      val_loss = loss_object(y_true=y_val, y_pred=val_logits)
      losses.append(val_loss)
      val_acc_metric(y_val, val_logits)
  return losses


model = base_model()

# Iterate over epochs.
epochs = 10
epochs_val_losses, epochs_train_losses = [], []
for epoch in range(epochs):
  print('Start of epoch %d' % (epoch,))
  
  losses_train = train_data_for_one_epoch()
  train_acc = train_acc_metric.result()

  losses_val = perform_validation()
  val_acc = val_acc_metric.result()

  losses_train_mean = np.mean(losses_train)
  losses_val_mean = np.mean(losses_val)
  epochs_val_losses.append(losses_val_mean)
  epochs_train_losses.append(losses_train_mean)

  print('\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc)))
  
  train_acc_metric.reset_states()
  val_acc_metric.reset_states()
```




